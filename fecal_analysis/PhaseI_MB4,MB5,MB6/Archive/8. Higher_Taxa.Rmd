---
title: "8. Higher_Taxa_Seq"
author: "Sevilla"
date: "October 12, 2019"
output: word_document
editor_options: 
  chunk_output_type: console
---
############### 
#Project Overview
To determine the performance of the sequencing controls, using TAR and TDR as metric value, at two taxonomic levels (family/genus).

#QIIME2 Pipeline
Version 1 of the pipeline was used (T:\DCEG\Projects\Microbiome\CGR_MB\MicroBiome\sc_scripts_qiime2_pipeline\working) for this test. Three runs were performed using the sample dataset on three different start days for a stochastic test. All runs were the same, and so R_Run1 was chosen for this analysis. Output data is stored on the T Drive (T:\DCEG\Projects\Microbiome\CGR_MB\MicroBiome\Project_NP0084_MB4)

#Sample Set
Samples included in this anaysis were the sequencing controls of NP0084-MB4, MB5, and MB6, merged into one run. This includes a 43 controls samples of seven different mock communities and two blank-types. Blank-types were subsequently removed.
-Project-ID: NP0084-MB4, NP0084-MB5, NP0084-MB6
-Sample-Cat: MSA, Zymo.Seq
-Sample-Descrip: MSA1000, MSA1001, MSA1002, MSA1003, D6305, D6306, D6311

#Analysis Overview
Phyloseq objects will be adapted from the analysis pipeline created for the fecal analysis of NP0084-MB4,MB5,MB6. This includes 1. Initial Processing which created the PhyloSeq objects from the three flowcells, and merged them together. OTU tables at two taxonomic levels (family and genus) were created during this step. and 2. Baseline_Seq which imported these taxonomic tables and merged the expected control information to created observed/expected data tables. Output data is stored on the T Drive (T:\DCEG\CGF\TechTransfer\Microbiome\Extraction\Optimization\Fecal\Fresh Fecal Optimization_2017.08\Phase I\Analysis\NP0084-MB4,5,6\R_HigherTaxa_Seq).

OTU tables will be compared below 

#Required libraries
```{r}
library("biom")
library(ape)
library(data.table)
library(ggplot2)
library(tm)
library(vegan)
library(tidyr)
library(cowplot)
library(scales)
library(phyloseq)
library(qiime2R)
library(tibble)
library(gridExtra)
source("sources/miseqR.R")
library(tidyverse)
source("sources/ggrare.R") #github library: https://rdrr.io/github/gauravsk/ranacapa/
library(compareDF)
```

#Input
```{r}
#Adapted from 1. Initital Processing code for fecal analysis
data_dir = c("T:\\DCEG\\Projects\\Microbiome\\CGR_MB\\MicroBiome\\")
project_name=c("Project_NP0084_MB4\\")
run_list=c("Stochastic_Run1","Stochastic_Run2","Stochastic_Run3")
manifest_name=c("NP0084-MB4_08_29_19_metadata_seq.txt")

output_location_sto=c("T:\\DCEG\\CGF\\TechTransfer\\Microbiome\\Extraction\\Optimization\\Fecal\\Fresh Fecal Optimization_2017.08\\Phase I\\Analysis\\NP0084-MB4,5,6\\R_Stochastic\\")
output_location_seq=c("T:\\DCEG\\CGF\\TechTransfer\\Microbiome\\Extraction\\Optimization\\Fecal\\Fresh Fecal Optimization_2017.08\\Phase I\\Analysis\\NP0084-MB4,5,6\\R_HigherTaxa_Seq\\")

sample_depth=100000
reference_db="greengenes"

#Adapted from 2. Baseline_Seq
control_location=c("T:\\DCEG\\CGF\\TechTransfer\\Microbiome\\Extraction\\Optimization\\Fecal\\Fresh Fecal Optimization_2017.08\\Phase I\\Controls\\Taxonomy\\")

#New Code Requirements
output_run_list=c("R_Run1","R_Run2","R_Run3")
taxa_levels=c("Family","Genus")
```

############### Adapted from 1. Initital Processing code for fecal analysis
#Directory Creation
```{r}
#Create directories - One for each output location, and the following subdirectories:
sub_create<-c("Data","Data\\Summary","Data\\Baseline","OTUs","Taxa","Graphs","Graphs\\Summary","Graphs\\Baseline")

for (a in output_run_list){
 dir.create(paste(output_location_sto,a,sep=""))
 
 for (b in sub_create){
  dir.create(paste(output_location_sto,a,"\\",b,sep=""))
 }
}

#Create one summary dir for misc. files
dir.create(paste(output_location_sto,"Summary",sep=""))
```

#Create PhySeq Objects
```{r}
count=1
for (a in run_list){
 run_data_dir = paste(data_dir,project_name,a,sep="")

 #Read OTUS
 otus<-read_qza(paste(run_data_dir,"\\Output\\qza_results\\table_dada2_qza_merged_parts_final\\table_dada2_merged_final_filt.qza",sep=""))
 
 #Read rooted tree
 tree<-read_qza(paste(run_data_dir,"\\Output\\qza_results\\phylogeny_qza_results\\rooted_tree.qza",sep=""))
 
 #Read Greengenes taxonomy file
 taxonomy<-read_qza(paste(run_data_dir,"\\Output\\qza_results\\taxonomy_qza_results\\taxonomy_",reference_db,".qza",sep=""))
 
 #Edit table
 tax_table<-do.call(rbind, strsplit(as.character(taxonomy$data$Taxon), "; "))
   colnames(tax_table)<-c("Kingdom","Phylum","Class","Order","Family","Genus","Species")
   rownames(tax_table)<-taxonomy$data$Feature.ID
 
 #read metadata
 metadata<-read.table(paste(run_data_dir,manifest_name,sep="\\"),sep='\t', header=T, row.names=1, comment="")
 
 #Create phylo object
 phyloname<-paste("physeq_complete",count,sep="")
 assign(phyloname,phyloseq(otu_table(otus$data, taxa_are_rows = T), phy_tree(tree$data), tax_table(tax_table), sample_data(metadata)))
 
 phylo_number=count #sets the number of phyloseq objects that will need to be merged downstream
 count=count+1
}
```

#Handle Sample Data 
```{r}
#Remove quarantined samples
count=1
for (a in run_list){
 phy_name<-paste("physeq_complete",count,sep="")
 
 assign(phy_name,subset_samples(get(phy_name),Vial.ID!="Quarantined"))
 count=count+1
}


#Output
count=1
for (a in run_list){
 phy_name<-paste("physeq_complete",count,sep="")
 
 s<-summary(sample_data(get(phy_name)))
 capture.output(s, file = paste(output_location_sto,output_run_list[count],"\\Data\\Summary\\summary_prefilter_",a,".txt",sep=""))
 o<-otu_table(get(phy_name))
 write.csv(o, file = paste(output_location_sto,output_run_list[count],"\\OTUs\\otu_table_",a,".csv",sep=""))
 
 count=count+1
}

remove(s,o)

```

#Prune taxonmoy
```{r}
#Prune for bacteria only
count=1
for (a in run_list){
 phy_name<-paste("physeq_complete",count,sep="")
 phy_filt_name<-paste("physeq_filt",count,sep="")
  
 assign(phy_filt_name,get(phy_name) %>%
  subset_taxa(
    Kingdom == "k__Bacteria" &
    Family  != "k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria" &
    Class   != "k__Bacteria; p__Cyanobacteria; c__Chloroplast"
  ))

 #Print summaries
 s<-summary(sample_data(get(phy_filt_name))$Sample.Cat)
 capture.output(s, file = paste(output_location_sto,output_run_list[count],"\\Data\\Summary\\summary_prefilter_sample_",a,".Cat.txt",sep=""))
 s<-summary(sample_data(get(phy_filt_name))$Sample.Descrip)
 capture.output(s, file = paste(output_location_sto,output_run_list[count],"\\Data\\Summary\\summary_prefilter_sample_",a,".Descrip.txt",sep=""))
 
 count=count+1
}

```

#Filter taxa >.001, filter samples with less than 10000 read
```{r}
#NOTE: Samples should have been filtered during Q2 pipeline - done to ensure this was completed
count=1
for (a in run_list){
 phy_filt_name<-paste("physeq_filt",count,sep="")
 
 assign(phy_filt_name, filter_taxa(get(phy_filt_name), function(x) mean(x) > 1e-2, TRUE))
 assign(phy_filt_name, prune_samples(sample_sums(get(phy_filt_name)) > 10000, get(phy_filt_name)))
 assign(phy_filt_name, get(phy_filt_name) %>% scale_reads(n=sample_depth))

 
 o<-otu_table(get(phy_name))
 write.csv(o, file = paste(output_location_sto,output_run_list[count],"\\OTUs\\otu_table_filt_",a,".csv",sep=""))

 count=count+1
}

```

#Merge OTU's into taxa tables
```{r}
count=1
for (a in output_run_list){
 
 phy_filt_name<-paste("physeq_filt",count,sep="")

 for (b in taxa_levels){
  
  # Create a factor corresponding to the taxalevel
  taxfac = factor(tax_table(get(phy_filt_name))[, b])
  
  # Tabulate the counts for each genera in each sample
  taxtab = apply(otu_table(get(phy_filt_name)), MARGIN = 2, function(x) {
     tapply(x, INDEX = taxfac, FUN = sum, na.rm = TRUE, simplify = TRUE)
  })
  taxtab<-as.data.frame(taxtab)
 
  for (c in rownames(taxtab)){
   taxa<-c
   
   if(b=="Genus"){
    
    if(!grepl("g__",taxa)){
    taxtab[c,b]<-"HigherGenus"
    } else{
    colname_update<-str_remove(c, "g__") #Need to remove the formatting of taxonmy for each viewing downstream
    }
   } else{
    if(!grepl("f__",taxa)){
    taxtab[c,b]<-"HigherFamily"
    } else{
    colname_update<-str_remove(c, "f__") #Need to remove the formatting of taxonmy for each viewing downstream
    }
    
   }
   
   colname_update<-gsub("[","",colname_update,fixed=TRUE) #fixed = TRUE disables regex
   colname_update<-gsub("]","",colname_update,fixed=TRUE)
   taxtab[c,b]<-colname_update
   
  }
  
  taxtab<-aggregate(taxtab[-ncol(taxtab)],by=list(taxtab[,b]),FUN="sum") #use -ncol since first col should not be summed
  
  #Check if there is a blank first column because any unassigned taxa (originally was "g__") will be blank due to above
  if(taxtab[1,1]==""){
  taxtab[1,1]<-"Unknown"
  }
  
  taxtab<-t(taxtab) #transpose for downstream metadata matching
  colnames(taxtab)<-taxtab[1,]
  taxtab<-taxtab[-1,]
  
  file_name =paste(output_location_sto,a,"\\Taxa\\taxa_Summary_",b,"_",run_list[count],".csv",sep="")
  write.csv(taxtab,file_name)
 
  metatab <- as.data.frame(sample_data(get(phy_filt_name)))
  file_name =paste(output_location_sto,a,"\\Taxa\\metadata_Summary_",b,"_",run_list[count],".csv",sep="")
  write.csv(metatab,file_name)
 }
 count=count+1
}
```

################## New Code
#Read in raw OTU tables
```{r}
count=1
for (a in output_run_list){
 
 otu_table_name<-paste("otu_",a,sep="")
 assign(otu_table_name,read.csv(paste(output_location_sto,a,"\\OTUs\\otu_table_filt_",run_list[count],".csv",sep="")))
 
 count=count+1
}

```

#Compare OTU tables
```{r}
#Determine if there are differences in the OTU read counts between the three stochastic runs 
#NOTE: This is after downsampling has occured
otu_diff_1to2 <- compare_df(otu_R_Run1,otu_R_Run2)
otu_diff_1to3 <- compare_df(otu_R_Run1,otu_R_Run3)
otu_diff_2to3 <- compare_df(otu_R_Run3,otu_R_Run3)

#Result: All of the tables are the same. No comparison needed at taxonomy levels.
```

################## Adapted from 2. Baseline_Seq
#NOTE: Will only use R_Run1 for the rest of the analysis, as all OTU information was the same, therefore taxonomic information will not vary
#Directory Creation
```{r}
#Create directories - One for each output location, and the following subdirectories:
sub_create<-c("Data", "OTUs", "Taxa", "Graphs")

for (a in taxa_levels){
 dir.create(paste(output_location_seq,a,sep=""))
 
 for (b in sub_create){
  dir.create(paste(output_location_seq,a,"\\",b,sep=""))
 }
}

#Create one summary dir for misc. files
dir.create(paste(output_location_seq,"Summary",sep=""))
```

#Taxonomy Reference
```{r}
#Read in controls taxonomy file, grouped by taxa
count=1
for (a in taxa_levels){
 expected_name<-paste("expected_taxa_",a,sep="")

 assign(expected_name,read.table(paste(control_location,"Taxonomy_",a,"_2019.txt",sep=""),sep="\t",header=TRUE,row.names=count+6))
 
 write.csv(get(expected_name),paste(output_location_seq,a,"\\Taxa\\Taxonomy",a,"_reference_2019.csv",sep=""))
 
 assign(expected_name,get(expected_name)[-1])
 count=count+1
}
```

#Store in taxa tables
```{r}
for (a in taxa_levels){
 tax_table_name<-paste("taxa",a,sep="_")
 assign(tax_table_name,read.csv(paste(output_location_sto,output_run_list[1],"\\Taxa\\taxa_Summary_",a,"_",run_list[1],".csv",sep=""),row.names=1))
 write.csv(get(tax_table_name),paste(output_location_seq,a,"\\Taxa\\Taxonomy",a,"_2019.csv",sep=""))
}

taxa_Family$Total_OTUs<-rowSums(taxa_Family)
taxa_Genus$Total_OTUs<-rowSums(taxa_Genus)
```

#Observed to Expected Counts
```{r}
#Will perform observed : expected counts on all seq mock communities
sample_list<-subset(sample_data(physeq_complete1),!sample_data(physeq_complete1)$Source.Descrip=="PCR.Blank")
sample_list<-subset(sample_list,!sample_list$Source.Descrip=="NTC.Blank")
sample_list<-rownames(sample_list) #contains only MSA and Zymo Controls

sample_list<-intersect(sample_list,rownames(taxa_Family)) #Removes any samples that did not meet required OTU threshold

obs_exp_tar_tdr<-data.frame()
observed_expected=0
expected_notobserved=0

for (a in taxa_levels){
 for (b in sample_list){
   source_temp <- metadata[b,"Source.Descrip"] 
   
   #need to create taxa lists specific for the control being analyzed
   taxa_list <- get(paste("expected_taxa_",a,sep=""))
   taxa_list <- row.names(subset(taxa_list,!(taxa_list[,source_temp]=="A")))
   taxa_temp <- get(paste("taxa_",a,sep=""))
   
   for (c in taxa_list){
    otu_count <- taxa_temp[b,c]
    
    if(!(is.na(otu_count) || is.null(otu_count) || otu_count==0)){
     observed_expected=observed_expected+1
    } else{
     expected_notobserved=expected_notobserved+1
    }
   }
   
   obs_exp_tar_tdr[b,"Source.Descrip"]<-metadata[b,"Source.Descrip"]
   obs_exp_tar_tdr[b,paste("Expected_",a,sep="")]<-length(taxa_list)

   obs_exp_tar_tdr[b,paste("TP_",a,sep="")]<-observed_expected
   #total-1 for total OTU column
   obs_exp_tar_tdr[b,paste("FP_",a,sep="")]<-sum(taxa_temp[b,]>1)-1-observed_expected
   obs_exp_tar_tdr[b,paste("FN_",a,sep="")]<-expected_notobserved
   
   obs_exp_tar_tdr[b,paste("TAR_",a,sep="")]<-obs_exp_tar_tdr[b,paste("TP_",a,sep="")]/(obs_exp_tar_tdr[b,paste("TP_",a,sep="")]+obs_exp_tar_tdr[b,paste("FP_",a,sep="")])
   obs_exp_tar_tdr[b,paste("TDR_",a,sep="")]<-obs_exp_tar_tdr[b,paste("TP_",a,sep="")]/(obs_exp_tar_tdr[b,paste("TP_",a,sep="")]+obs_exp_tar_tdr[b,paste("FN_",a,sep="")])
   
   

   observed_expected=0
   expected_notobserved=0
 }
}
 
#Average by Source.Descrip
source_types<-unique(subset(sample_data(physeq_complete1),!sample_data(physeq_complete1)$Source.Descrip=="PCR.Blank")$Source.Descrip)
source_types<-subset(source_types,!source_types==c("NTC.Blank","PCR.Bank"))

total_tp=0
total_fp=0
total_fn=0
total_expected=0
sample_count=0

for (a in taxa_levels){
 for (b in source_types){
   for (c in sample_list){
    
    source_temp <- metadata[c,"Source.Descrip"] 

    if(source_temp==b){
     total_expected<-obs_exp_tar_tdr[c,paste("Expected_",a,sep="")]
     total_tp<-obs_exp_tar_tdr[c,paste("TP_",a,sep="")]+total_tp
     total_fp<-obs_exp_tar_tdr[c,paste("FP_",a,sep="")]+total_fp
     total_fn<-obs_exp_tar_tdr[c,paste("FN_",a,sep="")]+total_fn
     
     sample_count=sample_count+1
    }
  }
  obs_exp_tar_tdr[b,"Source.Descrip"]<-b
  obs_exp_tar_tdr[b,paste("Expected_",a,sep="")]<-total_expected/sample_count
  obs_exp_tar_tdr[b,paste("TP_",a,sep="")]<-total_tp/sample_count
  obs_exp_tar_tdr[b,paste("FP_",a,sep="")]<-total_fp/sample_count
  obs_exp_tar_tdr[b,paste("FN_",a,sep="")]<-total_fp/sample_count
  obs_exp_tar_tdr[b,paste("TAR_",a,sep="")]<-total_tp/(total_tp+total_fp)
  obs_exp_tar_tdr[b,paste("TDR_",a,sep="")]<-total_tp/(total_tp+total_fn)
  
  
  total_tp=0
  total_fp=0
  total_fn=0
  total_expected=0
  sample_count=0
 }
}

obs_exp_tar_tdr

write.csv(obs_exp_tar_tdr,paste(output_location_seq,"Summary\\obs_exp_tar_tdr.csv",sep="")) 
```